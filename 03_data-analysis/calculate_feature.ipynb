{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from nltk.tree import *\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../01_data/preprocessedData/random_train_question.csv\").drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"correct_answer_text\"] = [ast.literal_eval(liste) for liste in df[\"correct_answer_text\"]]\n",
    "df[\"correct_answer_char_index\"] = [set(ast.literal_eval(liste)) for liste in df[\"correct_answer_char_index\"]]\n",
    "df[\"correct_answer_token_index\"] = [set(ast.literal_eval(liste)) for liste in df[\"correct_answer_token_index\"]]\n",
    "df[\"paragraph_tokens\"] = [set(ast.literal_eval(liste)) for liste in df[\"paragraph_tokens\"]]\n",
    "df[\"question_token\"] = [set(ast.literal_eval(liste)) for liste in df[\"question_token\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discard no answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53827"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[(df[\"correct_answer_text\"] != set()) & (df[\"correct_answer_token_index\"] != set())]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHP-Type & Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53827/53827 [00:06<00:00, 8878.73it/s] \n"
     ]
    }
   ],
   "source": [
    "WHP_type=[]\n",
    "WHP_token=[]\n",
    "\n",
    "for tree_string in tqdm(df[\"question_parse_tree\"]):\n",
    "    tree = Tree.fromstring(tree_string)\n",
    "    subtrees = list(tree.subtrees(filter=lambda x:x.label().startswith(\"WH\")))\n",
    "    if len(subtrees)>0:\n",
    "        WHP_type.append(subtrees[0].label())\n",
    "        WHP_token.append(\" \".join(subtrees[0].leaves()).lower())\n",
    "    else:\n",
    "        WHP_type.append(\"None\")\n",
    "        WHP_token.append(\"None\")\n",
    "\n",
    "df[\"WHP_type\"] = WHP_type\n",
    "df[\"WHP_token\"] = WHP_token\n",
    "\n",
    "whp_token_counter_df = df[\"WHP_token\"].value_counts()\n",
    "threshold = 100\n",
    "WHP_token_reduced = []\n",
    "for item in df[\"WHP_token\"]:\n",
    "    if whp_token_counter_df[item] > threshold:\n",
    "        WHP_token_reduced.append(item)\n",
    "    else:\n",
    "        WHP_token_reduced.append(\"other\")\n",
    "\n",
    "df[\"WHP_token_reduced\"] =WHP_token_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53827/53827 [00:00<00:00, 1186335.70it/s]\n"
     ]
    }
   ],
   "source": [
    "question_len=[]\n",
    "\n",
    "for question_token in tqdm(df[\"question_token\"]):\n",
    "    question_len.append(len(question_token))\n",
    "    \n",
    "df[\"question_len\"] = question_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Word ratio\n",
    "ratio of non stop word that did not appear in the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53827it [00:05, 9778.72it/s] \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "sw = set(stopwords.words(\"english\")).union({\"?\", \" \", \",\"})\n",
    "\n",
    "old_token_questions = []\n",
    "new_token_questions = []\n",
    "stopwords_questions = []\n",
    "\n",
    "old_token_ratios = []\n",
    "new_token_ratios = []\n",
    "stopwords_ratios = []\n",
    "\n",
    "\n",
    "for paragraph_token, question_token in tqdm(zip(list(df[\"paragraph_tokens\"]), list(df[\"question_token\"]))):\n",
    "    question_token = [unidecode(token.lower()) for token in question_token]\n",
    "    paragraph_token = [unidecode(token.lower()) for token in paragraph_token]\n",
    "    \n",
    "    stopwords_question = [token for token in question_token if token in sw]\n",
    "    old_token_question = [token for token in question_token if (token in paragraph_token) and (not token in sw)]\n",
    "    new_token_question = [token for token in question_token if (not token in paragraph_token) and (not token in sw)]\n",
    "    \n",
    "    old_token_ratios.append(len(old_token_question)/len(question_token))\n",
    "    new_token_ratios.append(len(new_token_question)/len(question_token))\n",
    "    stopwords_ratios.append(len(stopwords_question)/len(question_token))\n",
    "    \n",
    "    old_token_questions.append(len(old_token_question))\n",
    "    new_token_questions.append(len(new_token_question))\n",
    "    stopwords_questions.append(len(stopwords_question))\n",
    "    \n",
    "df[\"old_token_questions\"] = old_token_questions\n",
    "df[\"new_token_questions\"] = new_token_questions\n",
    "df[\"stopwords_questions\"] = stopwords_questions\n",
    "\n",
    "df[\"old_token_ratios\"] = old_token_ratios\n",
    "df[\"new_token_ratios\"] = new_token_ratios\n",
    "df[\"stopwords_ratios\"] = stopwords_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge_answer_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "merged_answer_indicies = []\n",
    "for i, row in df.iterrows():\n",
    "    merged_answer = set()\n",
    "    merged_answer.add(random.sample(row[\"correct_answer_token_index\"], 1)[0])\n",
    "\n",
    "    # TODO: There should be a propper merge here :D \n",
    "    #for answer_range in row[\"correct_answer_token_index\"]:\n",
    "        #print(\"merge\")\n",
    "\n",
    "    merged_answer_indicies.append(merged_answer)\n",
    "df[\"merged_answer_token_index\"] = merged_answer_indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## answer len feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "min_answer_len = []\n",
    "max_answer_len = []\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    answer_docs_len = []\n",
    "    for start, end in row[\"correct_answer_token_index\"]:\n",
    "        answer_docs_len.append(end-start)\n",
    "    min_answer_len.append(min(answer_docs_len))\n",
    "    max_answer_len.append(max(answer_docs_len))\n",
    "\n",
    "df[\"min_answer_len\"] = min_answer_len\n",
    "df[\"max_answer_len\"] = max_answer_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## answer types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ngram_counter(gram_spans):\n",
    "    return [tuple(t.text.lower() for t in gram_span) for gram_span in gram_spans]\n",
    "\n",
    "def first_token_ngram_counter(gram_spans):\n",
    "    return [tuple(t.text.lower() for t in gram_spans[0])]\n",
    "\n",
    "def pos_ngram_counter(gram_spans):\n",
    "    return [tuple(t.pos_ for t in gram_span) for gram_span in gram_spans]\n",
    "\n",
    "def first_pos_ngram_counter(gram_spans):\n",
    "    return [tuple(t.pos_ for t in gram_spans[0])]\n",
    "\n",
    "def tag_ngram_counter(gram_spans):\n",
    "    return [tuple(t.tag_ for t in gram_span) for gram_span in gram_spans]\n",
    "\n",
    "def first_tag_ngram_counter(gram_spans):\n",
    "    return [tuple(t.tag_ for t in gram_spans[0])]\n",
    "\n",
    "def ent_ngram_counter(gram_spans):\n",
    "    return [tuple(t.ent_type_ for t in gram_span) for gram_span in gram_spans]\n",
    "\n",
    "def first_ent_ngram_counter(gram_spans):\n",
    "    return [tuple(t.ent_type_ for t in gram_spans[0])]\n",
    "\n",
    "def contains_ner(all_answer_docs):\n",
    "    result_set = set()\n",
    "    for answer_doc in all_answer_docs:\n",
    "        result_set.update([token.ent_iob_ for token in answer_doc])\n",
    "    \n",
    "    return result_set != set('O')\n",
    "    \n",
    "def contains_ner_type(all_answer_docs):\n",
    "    result_set = set()\n",
    "    for answer_doc in all_answer_docs:\n",
    "        #print([token.text for token in answer_doc], \"--->\", [token.ent_type_ for token in answer_doc])\n",
    "        result_set.update([token.ent_type_ for token in answer_doc])\n",
    "    return result_set\n",
    "\n",
    "def contains_pos_type(all_answer_docs):\n",
    "    result_set = set()\n",
    "    for answer_doc in all_answer_docs:\n",
    "        #print([token.text for token in answer_doc], \"--->\", [token.ent_type_ for token in answer_doc])\n",
    "        result_set.update([token.pos_ for token in answer_doc])\n",
    "    return result_set\n",
    "\n",
    "def contains_tag_type(all_answer_docs):\n",
    "    result_set = set()\n",
    "    for answer_doc in all_answer_docs:\n",
    "        #print([token.text for token in answer_doc], \"--->\", [token.ent_type_ for token in answer_doc])\n",
    "        result_set.update([token.tag_ for token in answer_doc])\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53827it [26:27, 33.92it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_dict = defaultdict(list)\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    feature_dict[\"counter_answer\"].append(len(row[\"correct_answer_text\"]))\n",
    "    feature_dict[\"counter_unique_ answer\"].append(len(set(row[\"correct_answer_text\"])))\n",
    "    paragraph_context = nlp(row[\"paragraph_text\"])\n",
    "    all_answer_docs = []\n",
    "    for start, end in row[\"correct_answer_token_index\"]:\n",
    "        all_answer_docs.append(paragraph_context[start:end])\n",
    "    feature_dict[\"contains_ner\"].append(contains_ner(all_answer_docs))\n",
    "    feature_dict[\"contains_ner_type\"].append(contains_ner_type(all_answer_docs))\n",
    "    feature_dict[\"contains_pos_type\"].append(contains_pos_type(all_answer_docs))\n",
    "    feature_dict[\"contains_tag_type\"].append(contains_tag_type(all_answer_docs))\n",
    "    #for answer_doc in all_answer_docs:\n",
    "    #    for i in range(1,3):\n",
    "    #        gram_spans = list(textacy.extract.ngrams(answer_doc, i, filter_stops=False))\n",
    "    #        \n",
    "    #        if len(gram_spans)>0:\n",
    "    #            feature_dict[\"counter_{}\".format(i)].append(token_ngram_counter(gram_spans))\n",
    "    #            feature_dict[\"first_counter_{}\".format(i)].append(first_token_ngram_counter(gram_spans))#\n",
    "    #\n",
    "    #            feature_dict[\"pos_counter_{}\".format(i)].append(pos_ngram_counter(gram_spans))\n",
    "    #            feature_dict[\"first_pos_counter_{}\".format(i)].append(first_pos_ngram_counter(gram_spans))\n",
    "\n",
    "    #            feature_dict[\"tag_counter_{}\".format(i)].append(tag_ngram_counter(gram_spans))\n",
    "    #            feature_dict[\"first_tag_counter_{}\".format(i)].append(first_tag_ngram_counter(gram_spans))\n",
    "\n",
    "    #            feature_dict[\"ner_counter_{}\".format(i)].append(ent_ngram_counter(gram_spans))\n",
    "    #            feature_dict[\"first_ner_counter_{}\".format(i)].append(first_ent_ngram_counter(gram_spans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.reset_index(), pd.DataFrame(feature_dict).reset_index()], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"contains_ner_type_str\"] = [str(liste) for liste in df[\"contains_ner_type\"]]\n",
    "ner_counter_df = df[\"contains_ner_type_str\"].value_counts()\n",
    "\n",
    "contains_ner_type_str_reduced = []\n",
    "for item in df[\"contains_ner_type\"]:\n",
    "    \n",
    "    if len(item) < 2:\n",
    "        contains_ner_type_str_reduced.append(str(item))\n",
    "    else:\n",
    "        contains_ner_type_str_reduced.append(\"Other\")\n",
    "    \n",
    "    #if ner_counter_df[item] > threshold:\n",
    "    #    contains_ner_type_str_reduced.append(item)\n",
    "    #else:\n",
    "    #    contains_ner_type_str_reduced.append(\"Other\")\n",
    "    \n",
    "df[\"contains_ner_type_str_reduced\"] =contains_ner_type_str_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"random_train_question_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
